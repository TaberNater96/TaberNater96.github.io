<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Elijah Taber - Projects</title>    <link rel="stylesheet" href="../assets/css/normalize.css">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/project-content.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700&family=Open+Sans:wght@300;400;600&display=swap" rel="stylesheet"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/devicon.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/devicons/devicon@v2.15.1/devicon.min.css">
    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body class="projects-page">
    <!-- Navigation Bar -->
    <header class="site-header">
        <div class="container">
            <nav class="main-nav">
                <ul class="nav-menu">
                    <li><a href="../index.html">Home</a></li>
                    <li><a href="projects.html">Projects</a></li>
                    <li><a href="resume.html">Resume</a></li>
                    <li><a href="about.html">About</a></li>
                    <li><a href="contact.html">Contact</a></li>
                </ul>
                <button class="nav-toggle">
                    <span class="hamburger"></span>
                </button>
            </nav>
        </div>
    </header>

    <!-- Sidebar Navigation -->
    <aside class="projects-sidebar">
        <div class="projects-sidebar-container">
            <h3 class="projects-sidebar-title">Projects</h3>
            <ul>
                <li><a href="#atmoseer" class="active" data-section="atmoseer"><i class="fas fa-cloud"></i> AtmoSeer</a></li>
                <li><a href="#robi" data-section="robi"><i class="fas fa-robot"></i> RoBi</a></li>
                <li><a href="#portfolio" data-section="portfolio"><i class="fas fa-laptop-code"></i> Portfolio Site</a></li>
                <li><a href="#nfl-analysis" data-section="nfl-analysis"><i class="fas fa-football-ball"></i> NFL Analysis</a></li>
            </ul>
            <div class="projects-sidebar-social">
                <a href="https://github.com/TaberNater96" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
                <a href="https://www.linkedin.com/in/elijah-taber-17377b2a9/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
                <a href="mailto:boxcar2taber@yahoo.com" title="Email"><i class="fas fa-envelope"></i></a>
            </div>
        </div>
    </aside>

    <!-- Projects Section -->
    <section class="projects-section">
        <div class="container">
            <h2 class="projects-title">Projects</h2>
            
            <!-- AtmoSeer Project -->
            <div id="atmoseer" class="project-detail">
                <h3>AtmoSeer</h3>
                <p>
                    AtmoSeer originates from the objective of applying advanced deep learning techniques to the complex domain of atmospheric science, specifically for forecasting 
                    greenhouse gas concentrations. This project is a full-stack data science project, using advanced techniques in data engineering, data analysis, and deep learning 
                    to gather raw greenhouse data from NOAA, train a bidirectional recursive neural network on it, and then deploy it to a fully interactable front-end website that 
                    allows users to visually see the projection of global greenhouse gases. The aim is to provide a robust tool for understanding the past, and anticipating future 
                    changes in 4 of the most potent atmospheric gases like Carbon Dioxide (CO2), Methane (CH4), Nitrous Oxide (N2O), and Sulfur Hexafluoride (SF6). By making advanced 
                    data modeling accessible, AtmoSeer attempts to offer valuable insights for environmental monitoring and research, as there are plenty of tools and sources for 
                    looking at past atmospheric gas data, but limited on open-sourced time series models. 
                </p>
                <br>
                <p>
                    This pipeline begins with the acquisition of foundational data, also known as the “Extract” phase in the data engineering pipeline. Atmospheric measurements for the 
                    four principal greenhouse gases are sourced from the National Oceanic and Atmospheric Administration's Global Monitoring Laboratory (NOAA GML), a standard for high-quality 
                    atmospheric data gathered around the globe and given freely to the public. To provide ecological context, biomass data is integrated from NASA's Moderate Resolution 
                    Imaging Spectroradiometer (MODIS) and integrated during the feature engineering phase. 
                </p>
                <br>
                <p>
                    The first phase of the pipeline is the ETL phase, where the raw data extracted from the source, transformed into a usable format, and loaded into an organized data warehouse 
                    to analyze and use for training. These transformations include data cleansing (addressing anomalies, interpolating or removing missing values based on domain knowledge), 
                    normalization (scaling diverse measurements to comparable ranges to prevent numerical dominance by certain features), and structural reformatting. The refined data is then 
                    loaded into a dual-database system: a local PostgreSQL instance for agile development and intensive data manipulation, and a parallel data store in AWS, likely leveraging 
                    services such as DynamoDB, for persistence and scalable access.
                </p>
                <br>
                <p>
                    The first phase is the Extract, Transform, Load (ETL) phase. Raw data, once extracted from the NOAA GML and NASA MODIS sources, undergoes 
                    a sequence of programmatic transformations to prepare it for analytical and modeling tasks. These transformations include data cleansing operations, such as the identification 
                    and handling of anomalous readings, the imputation of missing values using statistical methods or time-series specific techniques, and the removal of erroneous data points. 
                    Numerical features are subjected to normalization, scaling them to a common range (e.g., 0 to 1 or standard normal distribution) to ensure that features with larger magnitudes 
                    do not disproportionately influence downstream algorithms. The data is also structurally reformatted, which involves reshaping datasets, aligning temporal resolutions, and 
                    creating a consistent schema across different sources. Following these transformations, the processed data is loaded into a dual-database architecture: a local PostgreSQL 
                    database is utilized for development, iterative analysis, and rapid querying, while a secondary data store is established in AWS for backup, persistence, and potentially 
                    scalable production access.
                </p>
                <br>
                <p>
                    Once the data is cleaned, transformed, and organized temporally, it is ready to be transformed into a format that an AI algorithm can understand. 
                    This is commonly referred to as the preprocessing phase. This involves the numerical encoding of any categorical features and a thorough validation of data integrity. A key 
                    operation here is the partitioning of data into training and validation subsets. To maintain the temporal integrity of the time series and to evaluate the model's predictive 
                    performance on data it has not seen, these splits are temporally ordered: the validation set comprises data points from a period subsequent to that of the training set. This 
                    simulates a realistic forecasting scenario and avoids data leakage.
                </p>
                <br>
                <p>
                    The core predictive engine of AtmoSeer is focused around the deep learning time series analysis model. This is not a monolithic model, but a framework that instantiates four 
                    specialized models, one for each target greenhouse gas. This gas-specific modeling allows for the capture of unique temporal dynamics and behaviors that are unique to CO2, 
                    CH4, N2O, and SF6.
                </p>
                <br>
                <p>
                    The AtmoSeer architecture is built using a Bidirectional Long Short-Term Memory (BiLSTM) network. BiLSTMs are a sophisticated variant of Recurrent Neural Networks (RNNs), adept 
                    at modeling both long-range dependencies and short-term trends, as there are often complex temporal patterns in sequential data. The "bidirectional" characteristic essentially 
                    means that input sequences are processed in both chronological and reverse-chronological order. This dual processing allows the model to incorporate information from both past 
                    and (relative) future time steps when constructing its internal representations, often leading to a richer understanding of the sequence context. While BiLSTMs can be computationally 
                    demanding, the purpose of this project was to focus on accurately modeling complex atmospheric gas trends, rather than a fast model that is less accurate. This tradeoff is a 
                    common concern in machine learning operations, where a heavy and computationally expensive model might be more accurate, but it will take longer to perform calculations. With 
                    this taken into account, logic for GPU acceleration was woven into the core architecture of AtmoSeer, which mitigates computational overhead during training and deployment by 
                    increasing efficiency by over 100x.
                </p>
                <br>
                <p>
                    AtmoSeer utilizes an advanced approach to processing environmental data, starting with normalization to make sure there is consistent scaling and stable learning. Like stated 
                    before, at its core lies a bidirectional processing system that analyzes sequences from both forward and backward directions simultaneously, capturing contextual atmospheric 
                    data from both past and future time points. This bidirectional architecture can include multiple stacked layers with strategic dropout techniques to prevent overfitting. The 
                    model's most innovative aspect is its attention mechanism, which functions like a spotlight to focus on the most relevant information while dimming less important signals, 
                    which creates a more focused representation of the input data. The architecture uses initialization strategies for different components, orthogonal initialization for 
                    recurrent weights to mitigate gradient issues common in such networks, and Xavier/Glorot uniform initialization for other weight matrices to maintain consistent signal 
                    strength. This design creates a robust system capable of extracting patterns from complex environmental data while avoiding common training pitfalls.
                </p>
                <br>
                <p>
                    The model's performance is optimized through Bayesian optimization, a sample-efficient approach for functions that are expensive to evaluate. This process uses a Gaussian 
                    Process (GP) surrogate model to approximate the objective function by mapping hyperparameters to performance metrics. The system utilizes acquisition functions such as 
                    Expected Improvement (EI) or Upper Confidence Bound (UCB) to determine which hyperparameter configurations to evaluate next. These functions mathematically balance exploration 
                    (sampling in high uncertainty regions) and exploitation (sampling in regions predicted to perform well). Each iteration updates the GP model with new hyperparameter-performance 
                    pairs, progressively refining the surrogate model's accuracy in predicting optimal configurations for parameters like learning rate, LSTM layer count, hidden dimensions, and dropout rates.
                </p>
                <br>
                <p>
                    Upon completion of the optimization process, the system identifies optimal hyperparameter configurations for each of the four gas detection models based on quantitative 
                    performance metrics. These configurations, including architectural specifications and learned parameters (weights and biases), are systematically archived in dedicated 
                    directory structures. The preservation mechanism typically involves PyTorch state dictionary files (.pth or .pt) containing model parameters, alongside JSON files that 
                    document the complete optimization trajectory, trial outcomes, and final parameter selections.
                </p>
                <br>
                <p>
                    The trained models are embedded within a Streamlit web application, structured around a main home page and dedicated sub-pages for individual greenhouse gases. Users can 
                    interact with the application in two primary ways: exploring historical gas concentration data by selecting specific dates or timeframes, and generating forecasts for future 
                    dates. For forecasting, the application retrieves the appropriate pre-trained AtmoSeer model and processes recent historical data as input. The forecasting mechanism employs 
                    an autoregressive approach where each prediction becomes input for subsequent predictions, while Monte Carlo sampling with noise injection provides uncertainty bounds around the central forecast.
                </p>
                <br>
                <p>
                    The application prioritizes accessibility through an intuitive interface with consistent navigation elements and custom styling across all pages. Each gas-specific page 
                    manages its own model interactions, data processing, and visualization components, presenting users with relevant information through Streamlit's interactive widgets. This
                    structure allows users to easily switch between different greenhouse gases while maintaining a consistent experience throughout the application.
                </p>
                <br>
                <p>
                    The application's deployment uses Streamlit Cloud's infrastructure, which directly connects to this project's GitHub repository to maintain synchronization with the latest 
                    codebase. This deployment strategy eliminates the need for manual server configuration or specialized DevOps knowledge, as Streamlit Cloud automatically handles the hosting 
                    environment, computational resources, and web serving capabilities required for both the frontend interface and backend model execution. All for free. The platform's integration 
                    with GitHub enables continuous deployment, where updates pushed to the repository are automatically reflected in the live application, while also providing essential services like 
                    authentication, HTTPS security, and application scaling without requiring additional infrastructure management from a development team.
                </p>
                <br>
                <p>
                    AtmoSeer was built to act as a bridge using time-series modeling with accessible environmental data visualization and forecasting. By combining advanced deep learning 
                    techniques with intuitive user interfaces, this system democratizes access to critical information about atmospheric greenhouse gas concentrations, supporting both scientific 
                    understanding and public awareness of environmental trends.
                </p>
            </div>
            
            <!-- RoBi Project -->
            <div id="robi" class="project-detail">
                <h3>RoBi</h3>
                <p>
                    Developed a hybrid sentiment analysis network using RoBERTa and BiLSTM. The architecture consisted of a novel sliding window approach to 
                    bypass max sequence lengths of modern transformers. Essentially allowing the BiLSTM to learn from an infinite number of tokens using RoBERTa's 
                    embedding matrix. A Bayesian Optimization algorithm was developed as a wrapper for RoBi's architecture, acting as a hyperparameter tuner. This 
                    algorithm used a Matérn kernel for Gaussian Regression to greatly improve RoBi's accuracy by leveraging probability to search a hyperparameter 
                    space based on prior trial results.
                </p>
            </div>
            
            <!-- Portfolio Website Project -->
            <div id="portfolio" class="project-detail">
                <h3>Portfolio Website</h3>
                <p>
                    This website itself was built from the ground up using HTML, CSS, and JavaScript. Designed with a focus on clean aesthetics, smooth animations, 
                    and user-friendly navigation. The layout adapts seamlessly across devices, while interactive elements enhance the user experience without 
                    sacrificing performance. Both the layout and visual themes were custom-designed from scratch, no templates or frameworks, allowing complete control 
                    over the structure, styling, and behavior of every component.
                </p>
            </div>
              <!-- NFL Analysis Project -->
            <div id="nfl-analysis" class="project-detail">
                <div class="project-content">
                    <div class="center-content">
                        <h1>Tackle Opportunity Window</h1>
                    </div>
                    
                    <div class="kaggle-button">
                        <a href="https://www.kaggle.com/code/godragons6/tackle-opportunity-window" target="_blank">
                            <img alt="Kaggle" title="View Competition Submission" src="https://kaggle.com/static/images/open-in-kaggle.svg">
                        </a>
                    </div>

                    <h2>Introduction</h2>
                    <p>
                        Imagine being a running back in the NFL and having a 6'4", 250 lb defensive linebacker, with a 4.39 40 yard dash, rapidly approaching you and tracking your every position. As an observer watching from above, it's hard to gain a solid understanding for what is actully happening down on the field, and what is going through the minds of the players in such a high stakes situation. Which player do you think will come out on top? Is it the player who is faster? Stronger? Smarter? A more nuanced analysis suggests that it is an amalgamation of these characteristics that defines the prowess of a professional football player. The main objective when the ball is snapped for every defensive player is to stop the ball carrier as soon as possible. Hence, the pivotal question arises: what are the paramount factors that facilitate this objective?
                    </p>

                    <p>
                        To address this, I have developed a unique metric, termed the <strong>Tackle Opportunity Window</strong>, or <strong>TOW</strong>. This metric quantifies the duration within which a defensive player can feasibly execute a tackle. The NFL contains some of the fastest and strongest human beings on the planet. On many occasions in football, a player on defense only has a fraction of a second to bring down a ball carrier before they are out of reach and must be brought down by a different defensive player. Therefore, an effective metric to assess a defender's tackling proficiency is to examine their actions during the critical moments preceding a tackle. A robust TOW score is indicative of a player's capacity to adeptly track, rapidly accelerate, and maintain a strategic proximity to the ball carrier. This proficiency is derived from an interplay of experience, skill, and physical capabilities, each contributing to the player's overall effectiveness on the field.
                    </p>

                    <h2>Theory</h2>
                    <p>
                        The functionality of this metric stems from calculating a dynamic variant of the Euclidean Distance across successive frames in order to determine the distance of the defensive players with respect to the ball carrier over each frame. This calculation is pivotal in ascertaining the fluctuating proximity of defensive players to the ball carrier within each frame. The variation in this distance, for each player, is contingent upon their relative Cartesian coordinates mapped onto the two-dimensional plane of the football field.
                    </p>

                    <h4>Relative Distance</h4>
                    <p>
                        The Euclidean distance \( d \) between a player and the ball carrier is calculated as:
                    </p>
                    
                    <div class="equation">
                        $$
                        d = \sqrt{(x_{\text{player}} - x_{\text{ball carrier}})^2 + (y_{\text{player}} - y_{\text{ball carrier}})^2}
                        $$
                    </div>

                    <h4>Threshold</h4>
                    <p>
                        Once the relative distance of each defensive player with respect to the ball carrier is known, it can be compared to the TOW threshold, which I have set to 2 yards. This threshold value can be adjusted if need be. If the distance is less than the threshold, the TOW counter will start.
                    </p>
                    
                    <div class="equation">
                        $$
                        wt = d \leq \theta
                        $$
                    </div>
                    
                    <p>Where:</p>
                    
                    <div class="equation">
                        $$
                        \theta = \text{threshold},
                        $$
                    </div>
                    
                    <div class="equation">
                        $$
                        wt = \text{within tackle threshold}
                        $$
                    </div>

                    <h4>Tackle Opportunity Window</h4>
                    <p>
                        For each consecutive frame that the defensive player remains within that threshold for the duration of the play, the counter will add a score of 1. Once the play is over, the algorithm will check for the largest number and assign that score to each player.
                    </p>
                    
                    <div class="equation">
                        $$
                        TOW (\theta) = \max_{\text{score}}\left(\sum_{i=1}^{n} wt_i - \min_{j \leq i}\left(\sum_{k=1}^{j} wt_k \cdot (1 - wt_j)\right)\right)
                        $$
                    </div>
                    
                    <p>
                        Due to the scale of this operation, transforming all of weeks (1 – 9) by looping over each row would be highly inefficient. Therefore I implemented a vectorized approach rather than deep nested loops to optimize the iteration process. Once the window is verified, the TOW counter is instantiated to create a cumulative score. This can be visualized using the following:
                    </p>
                    
                    <div class="equation">
                        $$
                        \text {TOW Score}_i \text{+=} 
                        \begin{cases} 
                        1 & \text{if } d_i \leq \theta \\
                        0 & \text{otherwise}
                        \end{cases}
                        $$
                    </div>
                    
                    <h4>Tackle Opportunity Window Ratio</h4>
                    <p>
                        The TOW score is a valuable metric for comparing players within an individual play. However, its effectiveness diminishes when assessing performances across multiple tackles over a nine-week period. This is because longer plays naturally lead to higher TOW scores, regardless of the actual time a tackler spends near the ball carrier. To address this, introducing a ratio-based system would offer a more accurate and fair representation of each player's performance in every distinct play.
                    </p>
                    
                    <div class="equation">
                        $$
                        \text{TOW Ratio} = {\text{TOW Score} \over \text{Total Frames Per Play}} 
                        $$
                    </div>
                      <p>
                        By leveraging geometric principles, the TOW metric provides a nuanced understanding of the spatial dynamics that influence and predict tackling outcomes in the game. To gain some insight into the overall distribution of the TOW ratio for the entire tracking dataset, I crafted a 2D density plot of tacklers with a non-zero TOW Ratio to illuminate how consistent tacklers are at maintaining a close proximity to their target.
                    </p>
                      <div class="project-image-container">
                        <img src="../assets/images/nfl/TOW_Ratio_Density_Plot.png" width="600" height="400" alt="TOW Ratio Density Plot">
                    </div>
                    
                    <p>
                        As one can see, the majority of tacklers stay within the tackle threshold for around 1/3 of the time, where the majority of the score is most likely at the end of the play.
                    </p>

                    <h2>Data Preprocessing</h2>
                    
                    <h4>Cross Mapping</h4>
                    <p>
                        In order to create a binary classification target variable that is aptly termed 'tackle', the dataset must be synthesized by extracting and amalgamating pertinent metrics from a plethora of distinct dataframes. This would be straight forward if the tracking datasets set one tackle value in the 'event' column, but the same events are recorded across each player. To circumvent this issue, and engineer a target variable, I cross referenced the tackles dataframe with my main tracking dataframe to map a binary indicator of 1 on the exact frame where a tackle occurred, but only on the specific nflId(s) that is responsible for the tackle. At this a point the tackler(s), ball carrier, and relative spatial separation from the two are known, allowing for rudimentary trigonometry computations to determine if the threshold has been passed, initiating the TOW counter.
                    </p>
                    
                    <h4>Data Normalization and Transformation</h4>
                    <p>
                        The tracking datasets underpinning this analysis is extensive, containing 9 CSV files that collectively encompass over 12 million data entries. A critical aspect of this dataset is the 'gameId' attribute, which serves as a distinct identifier for each game. However, the 'playId' attribute, while unique within the context of a single game, is not globally unique across different games. This implies that while each game is associated with a distinct set of 'playId' numbers, these identifiers are recycled across different games. This makes things difficult when trying to group the playId's by their unique values in order to filter out a tackle without combining mutiple separate plays into one due to their ID's being the same value. To address this complexity, a transformation was implemented by numerically encoding 'playId' values, culminating in the creation of a new, distinct attribute. This attribute iterates through the entire dataset, assigning a unique value to each play. This process ensures the preservation of the dataset's structural integrity and chronological coherence.
                    </p>
                    
                    <p>
                        A good way to visualize the data phenomena is by looking at the tackles dataframe, sorted by playId. Notice recycled values in the playId column:
                    </p>
                      <div class="project-image-container">
                        <img src="../assets/images/nfl/Tackles_Dataframe_Example.png" width="600" height="400" alt="Tackles Dataframe Example">
                    </div>

                    <h2>Deep Neural Network</h2>
                    
                    <h4>Predictive Tracking Indicators</h4>
                    <p>
                        Owing to the intrinsic capabilities of a neural network, particularly its adeptness in detecting nuanced variations, the direction and orientation of each player were pivotal in enabling the model to discern and adapt to subtle shifts. These shifts are essential for the model to recognize and adhere to an emergent pattern, as the features exhibit significant, yet controlled, variation across successive frames. This variation is not arbitrary, but rather demonstrative of a tackler pursuing their target with precision. The controlled variability within these features provides the model with critical data points, allowing it to effectively learn and predict the dynamics of a tackler's movement in relation to their target. Visualizing the distribution of each player's orientation and direction in the EDA phase, and noticing the non-random variation, is what gave rise to the idea of focusing on this specific concept in parallel with the tackle opportunity window.
                    </p>
                      <div class="project-image-container">
                        <img src="../assets/images/nfl/Polar_Histogram.png" width="600" height="600" alt="Polar Histogram">
                    </div>
                    
                    <h4>Architecture</h4>
                    <p>
                        When the target variable is split, its training set is used to compute the class weight of the network using balanced parameters to address the disproportionality of the tackle ratio. The training and test sets are then split into categorical and continuous variables, then embedded into the network to capture underlying connections by mapping integers to dense vectors. The network is then compiled using the loss function as binary-crossentropy, with a focus on tracking performance. Since overfitting is a common issue in neural networks, the features were kept to a necessary minimum. The Adam algorithm uses a tunable learning rate to ensure this hyperparameter is set to an optimal frequency. The architecture framework I implemented analyzes the validation loss of each epoch and then automatically extracts the highest performing parameters. Validation loss is favored over validation accuracy due to the nature of the binary output layer and the overall balance the target variable.
                    </p>

                    <h2>Results</h2>
                    
                    <h4>Tackle Opportunity Window & Orientation In Context</h4>
                    <p>
                        To provide a deeper understanding of how this model operates in practice, consider the following play as an illustrative example. The yellow radius around the ball illustrates the TOW threshold while the black radius around the defenders represents their respective tackle probability density. Notice how the densities for Tyrann Mathieu and Marshon Lattimore fluctuate, influenced by their orientation and distance to the ball carrier. These two factors are critical. As soon as the TOW threshold is breached, their probability densities spike.
                    </p>
                      <div class="project-image-container">
                        <img src="../assets/images/nfl/Marshon_and_Tyrann.png" width="800" height="100" class="player-image" alt="Marshon and Tyrann">
                    </div>
                    
                    <div class="project-image-container">
                        <img src="../assets/images/nfl/TOW_Animation.gif" width="800" height="400" alt="TOW Animation">
                    </div>
                    
                    <h4>Time Series Analysis</h4>
                    <p>
                        Defenders who can maintain a consistent tackle opportunity window by adjusting their orientation and acceleration on a moment's notice can double or triple their chances of securing a tackle. This is due to the fast-paced environment of the NFL and how split second decisions determine the outcome of a play. The following graph illustrates this concept by analyzing how Tyrann Mathieu and Marshon Lattimore's probabilities fluctuate as their orientation and distance to the ball carrier change.
                    </p>
                      <div class="project-image-container">
                        <img src="../assets/images/nfl/Marshon_and_Tyrann.png" width="800" height="100" class="player-image" alt="Marshon and Tyrann">
                    </div>
                    
                    <div class="project-image-container">
                        <img src="../assets/images/nfl/TOW_Plot_Animation.gif" width="800" height="400" alt="TOW Plot Animation">
                    </div>
                    
                    <h4>Angle of Pursuit</h4>
                    <p>
                        The following play by Yetur Gross-Matos, an outside linebacker for the Carolina Panthers, illustrates how an angle of pursuit is invaluable in tackling a ball carrier that is already on the run at full speed. His ability to track, accelerate, and properly angle himself, resulting in a crucial tackle, exemplifies his effort in not giving up on the play.
                    </p>
                      <div class="project-image-container">
                        <img src="../assets/images/nfl/Gross_Matos.png" width="600" height="100" class="player-image" alt="Gross-Matos">
                    </div>
                    
                    <div class="project-image-container">
                        <img src="../assets/images/nfl/Angle_of_Pursuit.gif" width="800" height="400" alt="Angle of Pursuit Animation">
                    </div>

                    <h2>Rankings</h2>
                    <p>
                        The tackle opportunity window excels at demonstrating how efficient defenders are at tracking and maintaining a consistent proximity to their respective target. This parameter, however, necessitates contextual interpretation, as the positional roles of players inherently present divergent scopes for target tracking, contingent upon their spatial deployment and the dynamic opposition presented by offensive counterparts. This results in some players receiving more resistance than others. Therefore, I have developed a tripartite ranking system, categorizing defensive players' tracking capabilities in accordance with their positional classifications.
                    </p>
                      <div class="project-image-container">
                        <img src="../assets/images/nfl/DL_Rankings.png" width="800" height="600" class="rankings-image" alt="DL Rankings">
                    </div>
                    
                    <div class="project-image-container">
                        <img src="../assets/images/nfl/Linebacker_Rankings.png" width="800" height="600" class="rankings-image" alt="Linebacker Rankings">
                    </div>
                    
                    <div class="project-image-container">
                        <img src="../assets/images/nfl/DB_Rankings.png" width="800" height="600" class="rankings-image" alt="DB Rankings">
                    </div>

                    <h2>Conclusion</h2>
                    <p>
                        The deep learning network I implemented demonstrated solid proficiency in identifying not just the instances where defensive players are narrowing the gap with the ball carrier, but also in recognizing their alignment and predicting their optimal positioning for successful tackles. It was observed that mere proximity to the ball carrier is insufficient for a high probability of tackling; proper orientation and strategic positioning are crucial. This insight is particularly relevant in the NFL, where the combination of speed, strength, and agility often overshadows the necessity for tactical placement and orientation, especially when players are outside the immediate tackle threshold.
                    </p>
                    
                    <p>
                        The introduction of the 'tackle opportunity window' metric marks a significant advancement in evaluating defensive player performance. This metric offers a novel perspective by illustrating how effectively individual players track, accelerate towards, and maintain strategic proximity to the ball carrier. It also highlights their ability to anticipate and align themselves accurately with the future movements of the ball carrier. This interplay is a synthesis of rigorous training, experience, and overall intuition that characterizes a professional football player. Overall, this study provides valuable insights into the complex dynamics of defensive play in the NFL, underscoring the intricate balance between physical prowess and tactical intelligence.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="site-footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-logo">
                    <a href="../index.html">Elijah Taber</a>
                </div>
                <div class="footer-links">
                    <ul>
                        <li><a href="../index.html#projects">Projects</a></li>
                        <li><a href="resume.html">Resume</a></li>
                        <li><a href="about.html">About</a></li>
                        <li><a href="contact.html">Contact</a></li>
                    </ul>
                </div>
                <div class="footer-social">
                    <a href="https://github.com/TaberNater96"><i class="fab fa-github"></i></a>
                    <a href="https://www.linkedin.com/in/elijah-taber-17377b2a9/"><i class="fab fa-linkedin"></i></a>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 Elijah Taber. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <!-- JavaScript -->
    <script src="../assets/js/main.js"></script>
</body>
</html>