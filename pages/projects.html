<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Elijah Taber - Projects</title>
    <link rel="stylesheet" href="../assets/css/normalize.css">
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700&family=Open+Sans:wght@300;400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/devicon.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/devicons/devicon@v2.15.1/devicon.min.css">
</head>
<body class="projects-page">
    <!-- Navigation Bar -->
    <header class="site-header">
        <div class="container">
            <nav class="main-nav">
                <ul class="nav-menu">
                    <li><a href="../index.html">Home</a></li>
                    <li><a href="projects.html">Projects</a></li>
                    <li><a href="resume.html">Resume</a></li>
                    <li><a href="about.html">About</a></li>
                    <li><a href="contact.html">Contact</a></li>
                </ul>
                <button class="nav-toggle">
                    <span class="hamburger"></span>
                </button>
            </nav>
        </div>
    </header>

    <!-- Sidebar Navigation -->
    <aside class="projects-sidebar">
        <div class="projects-sidebar-container">
            <h3 class="projects-sidebar-title">Projects</h3>
            <ul>
                <li><a href="#atmoseer" class="active" data-section="atmoseer"><i class="fas fa-cloud"></i> AtmoSeer</a></li>
                <li><a href="#robi" data-section="robi"><i class="fas fa-robot"></i> RoBi</a></li>
                <li><a href="#portfolio" data-section="portfolio"><i class="fas fa-laptop-code"></i> Portfolio Site</a></li>
                <li><a href="#nfl-analysis" data-section="nfl-analysis"><i class="fas fa-football-ball"></i> NFL Analysis</a></li>
            </ul>
            <div class="projects-sidebar-social">
                <a href="https://github.com/TaberNater96" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
                <a href="https://www.linkedin.com/in/elijah-taber-17377b2a9/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
                <a href="mailto:boxcar2taber@yahoo.com" title="Email"><i class="fas fa-envelope"></i></a>
            </div>
        </div>
    </aside>

    <!-- Projects Section -->
    <section class="projects-section">
        <div class="container">
            <h2 class="projects-title">Projects</h2>
            
            <!-- AtmoSeer Project -->
            <div id="atmoseer" class="project-detail">
                <h3>AtmoSeer</h3>
                <p>
                    AtmoSeer originates from the objective of applying advanced deep learning techniques to the complex domain of atmospheric science, specifically for forecasting 
                    greenhouse gas concentrations. This project is a full-stack data science project, using advanced techniques in data engineering, data analysis, and deep learning 
                    to gather raw greenhouse data from NOAA, train a bidirectional recursive neural network on it, and then deploy it to a fully interactable front-end website that 
                    allows users to visually see the projection of global greenhouse gases. The aim is to provide a robust tool for understanding the past, and anticipating future 
                    changes in 4 of the most potent atmospheric gases like Carbon Dioxide (CO2), Methane (CH4), Nitrous Oxide (N2O), and Sulfur Hexafluoride (SF6). By making advanced 
                    data modeling accessible, AtmoSeer attempts to offer valuable insights for environmental monitoring and research, as there are plenty of tools and sources for 
                    looking at past atmospheric gas data, but limited on open-sourced time series models. 
                </p>
                <br>
                <p>
                    This pipeline begins with the acquisition of foundational data, also known as the “Extract” phase in the data engineering pipeline. Atmospheric measurements for the 
                    four principal greenhouse gases are sourced from the National Oceanic and Atmospheric Administration's Global Monitoring Laboratory (NOAA GML), a standard for high-quality 
                    atmospheric data gathered around the globe and given freely to the public. To provide ecological context, biomass data is integrated from NASA's Moderate Resolution 
                    Imaging Spectroradiometer (MODIS) and integrated during the feature engineering phase. 
                </p>
                <br>
                <p>
                    The first phase of the pipeline is the ETL phase, where the raw data extracted from the source, transformed into a usable format, and loaded into an organized data warehouse 
                    to analyze and use for training. These transformations include data cleansing (addressing anomalies, interpolating or removing missing values based on domain knowledge), 
                    normalization (scaling diverse measurements to comparable ranges to prevent numerical dominance by certain features), and structural reformatting. The refined data is then 
                    loaded into a dual-database system: a local PostgreSQL instance for agile development and intensive data manipulation, and a parallel data store in AWS, likely leveraging 
                    services such as DynamoDB, for persistence and scalable access.
                </p>
                <br>
                <p>
                    The first phase is the Extract, Transform, Load (ETL) phase. Raw data, once extracted from the NOAA GML and NASA MODIS sources, undergoes 
                    a sequence of programmatic transformations to prepare it for analytical and modeling tasks. These transformations include data cleansing operations, such as the identification 
                    and handling of anomalous readings, the imputation of missing values using statistical methods or time-series specific techniques, and the removal of erroneous data points. 
                    Numerical features are subjected to normalization, scaling them to a common range (e.g., 0 to 1 or standard normal distribution) to ensure that features with larger magnitudes 
                    do not disproportionately influence downstream algorithms. The data is also structurally reformatted, which involves reshaping datasets, aligning temporal resolutions, and 
                    creating a consistent schema across different sources. Following these transformations, the processed data is loaded into a dual-database architecture: a local PostgreSQL 
                    database is utilized for development, iterative analysis, and rapid querying, while a secondary data store is established in AWS for backup, persistence, and potentially 
                    scalable production access.
                </p>
                <br>
                <p>
                    Once the data is cleaned, transformed, and organized temporally, it is ready to be transformed into a format that an AI algorithm can understand. 
                    This is commonly referred to as the preprocessing phase. This involves the numerical encoding of any categorical features and a thorough validation of data integrity. A key 
                    operation here is the partitioning of data into training and validation subsets. To maintain the temporal integrity of the time series and to evaluate the model's predictive 
                    performance on data it has not seen, these splits are temporally ordered: the validation set comprises data points from a period subsequent to that of the training set. This 
                    simulates a realistic forecasting scenario and avoids data leakage.
                </p>
                <br>
                <p>
                    The core predictive engine of AtmoSeer is focused around the deep learning time series analysis model. This is not a monolithic model, but a framework that instantiates four 
                    specialized models, one for each target greenhouse gas. This gas-specific modeling allows for the capture of unique temporal dynamics and behaviors that are unique to CO2, 
                    CH4, N2O, and SF6.
                </p>
                <br>
                <p>
                    The AtmoSeer architecture is built using a Bidirectional Long Short-Term Memory (BiLSTM) network. BiLSTMs are a sophisticated variant of Recurrent Neural Networks (RNNs), adept 
                    at modeling both long-range dependencies and short-term trends, as there are often complex temporal patterns in sequential data. The "bidirectional" characteristic essentially 
                    means that input sequences are processed in both chronological and reverse-chronological order. This dual processing allows the model to incorporate information from both past 
                    and (relative) future time steps when constructing its internal representations, often leading to a richer understanding of the sequence context. While BiLSTMs can be computationally 
                    demanding, the purpose of this project was to focus on accurately modeling complex atmospheric gas trends, rather than a fast model that is less accurate. This tradeoff is a 
                    common concern in machine learning operations, where a heavy and computationally expensive model might be more accurate, but it will take longer to perform calculations. With 
                    this taken into account, logic for GPU acceleration was woven into the core architecture of AtmoSeer, which mitigates computational overhead during training and deployment by 
                    increasing efficiency by over 100x.
                </p>
                <br>
                <p>
                    AtmoSeer utilizes an advanced approach to processing environmental data, starting with normalization to make sure there is consistent scaling and stable learning. Like stated 
                    before, at its core lies a bidirectional processing system that analyzes sequences from both forward and backward directions simultaneously, capturing contextual atmospheric 
                    data from both past and future time points. This bidirectional architecture can include multiple stacked layers with strategic dropout techniques to prevent overfitting. The 
                    model's most innovative aspect is its attention mechanism, which functions like a spotlight to focus on the most relevant information while dimming less important signals, 
                    which creates a more focused representation of the input data. The architecture uses initialization strategies for different components, orthogonal initialization for 
                    recurrent weights to mitigate gradient issues common in such networks, and Xavier/Glorot uniform initialization for other weight matrices to maintain consistent signal 
                    strength. This design creates a robust system capable of extracting patterns from complex environmental data while avoiding common training pitfalls.
                </p>
                <br>
                <p>
                    The model's performance is optimized through Bayesian optimization, a sample-efficient approach for functions that are expensive to evaluate. This process uses a Gaussian 
                    Process (GP) surrogate model to approximate the objective function by mapping hyperparameters to performance metrics. The system utilizes acquisition functions such as 
                    Expected Improvement (EI) or Upper Confidence Bound (UCB) to determine which hyperparameter configurations to evaluate next. These functions mathematically balance exploration 
                    (sampling in high uncertainty regions) and exploitation (sampling in regions predicted to perform well). Each iteration updates the GP model with new hyperparameter-performance 
                    pairs, progressively refining the surrogate model's accuracy in predicting optimal configurations for parameters like learning rate, LSTM layer count, hidden dimensions, and dropout rates.
                </p>
                <br>
                <p>
                    Upon completion of the optimization process, the system identifies optimal hyperparameter configurations for each of the four gas detection models based on quantitative 
                    performance metrics. These configurations, including architectural specifications and learned parameters (weights and biases), are systematically archived in dedicated 
                    directory structures. The preservation mechanism typically involves PyTorch state dictionary files (.pth or .pt) containing model parameters, alongside JSON files that 
                    document the complete optimization trajectory, trial outcomes, and final parameter selections.
                </p>
                <br>
                <p>
                    The trained models are embedded within a Streamlit web application, structured around a main home page and dedicated sub-pages for individual greenhouse gases. Users can 
                    interact with the application in two primary ways: exploring historical gas concentration data by selecting specific dates or timeframes, and generating forecasts for future 
                    dates. For forecasting, the application retrieves the appropriate pre-trained AtmoSeer model and processes recent historical data as input. The forecasting mechanism employs 
                    an autoregressive approach where each prediction becomes input for subsequent predictions, while Monte Carlo sampling with noise injection provides uncertainty bounds around the central forecast.
                </p>
                <br>
                <p>
                    The application prioritizes accessibility through an intuitive interface with consistent navigation elements and custom styling across all pages. Each gas-specific page 
                    manages its own model interactions, data processing, and visualization components, presenting users with relevant information through Streamlit's interactive widgets. This
                    structure allows users to easily switch between different greenhouse gases while maintaining a consistent experience throughout the application.
                </p>
                <br>
                <p>
                    The application's deployment uses Streamlit Cloud's infrastructure, which directly connects to this project's GitHub repository to maintain synchronization with the latest 
                    codebase. This deployment strategy eliminates the need for manual server configuration or specialized DevOps knowledge, as Streamlit Cloud automatically handles the hosting 
                    environment, computational resources, and web serving capabilities required for both the frontend interface and backend model execution. All for free. The platform's integration 
                    with GitHub enables continuous deployment, where updates pushed to the repository are automatically reflected in the live application, while also providing essential services like 
                    authentication, HTTPS security, and application scaling without requiring additional infrastructure management from a development team.
                </p>
                <br>
                <p>
                    AtmoSeer was built to act as a bridge using time-series modeling with accessible environmental data visualization and forecasting. By combining advanced deep learning 
                    techniques with intuitive user interfaces, this system democratizes access to critical information about atmospheric greenhouse gas concentrations, supporting both scientific 
                    understanding and public awareness of environmental trends.
                </p>
            </div>
            
            <!-- RoBi Project -->
            <div id="robi" class="project-detail">
                <h3>RoBi</h3>
                <p>
                    Developed a hybrid sentiment analysis network using RoBERTa and BiLSTM. The architecture consisted of a novel sliding window approach to 
                    bypass max sequence lengths of modern transformers. Essentially allowing the BiLSTM to learn from an infinite number of tokens using RoBERTa's 
                    embedding matrix. A Bayesian Optimization algorithm was developed as a wrapper for RoBi's architecture, acting as a hyperparameter tuner. This 
                    algorithm used a Matérn kernel for Gaussian Regression to greatly improve RoBi's accuracy by leveraging probability to search a hyperparameter 
                    space based on prior trial results.
                </p>
            </div>
            
            <!-- Portfolio Website Project -->
            <div id="portfolio" class="project-detail">
                <h3>Portfolio Website</h3>
                <p>
                    This website itself was built from the ground up using HTML, CSS, and JavaScript. Designed with a focus on clean aesthetics, smooth animations, 
                    and user-friendly navigation. The layout adapts seamlessly across devices, while interactive elements enhance the user experience without 
                    sacrificing performance. Both the layout and visual themes were custom-designed from scratch, no templates or frameworks, allowing complete control 
                    over the structure, styling, and behavior of every component.
                </p>
            </div>
            
            <!-- NFL Analysis Project -->
            <div id="nfl-analysis" class="project-detail">
                <h3>Tackle Opportunity Window</h3>
                <p>
                    Designed a completely novel metric, along with a neural network and animation, that quantified the probability of a tackle 
                    occurring on a specific frame based on orientation and proximity. Received an honorable mention in the 2024 "Big Data Bowl", hosted by the NFL, 
                    based on the quality of my model and presentation. The notebook submission received more upvotes than 95% of the other competitors, 
                    consisting of hundreds of professional data scientists around the world.
                </p>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="site-footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-logo">
                    <a href="../index.html">Elijah Taber</a>
                </div>
                <div class="footer-links">
                    <ul>
                        <li><a href="../index.html#projects">Projects</a></li>
                        <li><a href="resume.html">Resume</a></li>
                        <li><a href="about.html">About</a></li>
                        <li><a href="contact.html">Contact</a></li>
                    </ul>
                </div>
                <div class="footer-social">
                    <a href="https://github.com/TaberNater96"><i class="fab fa-github"></i></a>
                    <a href="https://www.linkedin.com/in/elijah-taber-17377b2a9/"><i class="fab fa-linkedin"></i></a>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 Elijah Taber. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <!-- JavaScript -->
    <script src="../assets/js/main.js"></script>
</body>
</html>